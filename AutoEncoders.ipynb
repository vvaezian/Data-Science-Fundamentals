{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1)                 2         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "  Dense(1, input_shape=(2,), use_bias=False),\n",
    "  Dense(2, use_bias=False)\n",
    "])\n",
    "\n",
    "model.compile(loss='mean_absolute_error')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1: Not all features are independent of each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below data has two features, the second one is dependent on the first (it's 5 times bigger than the first)\n",
    "\n",
    "train1 = []\n",
    "import random\n",
    "for _ in range(1000000):\n",
    "  r = random.random()\n",
    "  train1.append((r, 100 * r))\n",
    "\n",
    "train1 = np.array(train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28125/28125 [==============================] - 36s 1ms/step - loss: 1.3486 - val_loss: 0.0184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x282a55bf0d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since this is a autoEncoder, the input and output data are the same\n",
    "model.fit(train1, train1, validation_split=0.1)  # default batch_size=32, so the number of iterations = 900000 / 32 = 28125\n",
    "\n",
    "# \"loss\" is calculated after each bactch for that batch. \"val_loss\" is calculated at the end for the whole val set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
       " array([[-1.6856214 ],\n",
       "        [-0.71987677]], dtype=float32)>,\n",
       " <tf.Variable 'dense_1/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[-0.01384243, -1.3580726 ]], dtype=float32)>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output weights -0.01384243, -1.3580726 show that the network has learned to output the bottleneck value and 100 x bottleneck value.  \n",
    "On the other hand, -1.685 * x + -0.719 * (100 * x) = -73.585 * x, and -0.013 * (-73.585 * x) = 0.95 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.50990885, 50.026848  ]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([(0.5, 50)])\n",
    "model.predict([a])\n",
    "\n",
    "# we see that it was able to reconstruct the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2: All features are independent of each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28125/28125 [==============================] - 36s 1ms/step - loss: 15.6407 - val_loss: 15.6183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x282ae0a3e20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2 = []\n",
    "import random\n",
    "for _ in range(1000000):\n",
    "  r1 = random.random() * 100  # we multiply by 100 to get the same range of numbers as scenario 1\n",
    "  r2 = random.random() * 100\n",
    "  train2.append((r1, r2))\n",
    "\n",
    "train2 = np.array(train2)\n",
    "\n",
    "model.fit(train2, train2, validation_split=0.1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that loss cannot be lowered further and is much higher than the first scenario.  \n",
    "This is due to the fact that the input couldn't be compressed (as was done in the first scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_2/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
       " array([[-1.2997323 ],\n",
       "        [-0.00222956]], dtype=float32)>,\n",
       " <tf.Variable 'dense_3/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[-0.76876694, -0.5793358 ]], dtype=float32)>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of val_loss: 15.8892**  \n",
    "The average distance between two points in 0-100 interval is 33.3 (below).  \n",
    "val_loss of 15.8892 shows that the network has reconstructed one of the elements (almost 0 loss) and for the other element it hasn't learned anything (so almost 33.3 loss). The average of these two would be 16.6 (in the case 0 loss for 1 element and total loss for the other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 90ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.049759 , 3.0566564]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([(4, 20)])\n",
    "model.predict([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.33"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = []\n",
    "for i in range(100):\n",
    "  for j in range(100):\n",
    "    out.append(abs(i - j))\n",
    "np.mean(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
